{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "> Note: sometimes your answer doesn't match one of \n",
    "> the options exactly. That's fine. \n",
    "> Select the option that's closest to your solution.\n",
    "\n",
    "\n",
    "In this homework, we will use the Bank Marketing dataset. Download it from [here](https://archive.ics.uci.edu/static/public/222/bank+marketing.zip).\n",
    "\n",
    "You can do it with `wget`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/static/public/222/bank+marketing.zip\n",
    "!unzip bank+marketing.zip\n",
    "!unzip bank.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need `bank-full.csv`.\n",
    "\n",
    "In this dataset the target variable is `y` variable - has the client subscribed a term deposit or not. \n",
    "\n",
    "### Dataset preparation\n",
    "\n",
    "For the rest of the homework, you'll need to use only these columns:\n",
    "\n",
    "* `'age'`,\n",
    "* `'job'`,\n",
    "* `'marital'`,\n",
    "* `'education'`,\n",
    "* `'balance'`,\n",
    "* `'housing'`,\n",
    "* `'contact'`,\n",
    "* `'day'`,\n",
    "* `'month'`,\n",
    "* `'duration'`,\n",
    "* `'campaign'`,\n",
    "* `'pdays'`,\n",
    "* `'previous'`,\n",
    "* `'poutcome'`,\n",
    "* `'y'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_list = [\n",
    "    'age',\n",
    "    'job',\n",
    "    'marital',\n",
    "    'education',\n",
    "    'balance',\n",
    "    'housing',\n",
    "    'contact',\n",
    "    'day',\n",
    "    'month',\n",
    "    'duration',\n",
    "    'campaign',\n",
    "    'pdays',\n",
    "    'previous',\n",
    "    'poutcome',\n",
    "    'y'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"bank-full.csv\", delimiter=\";\")\n",
    "df = df[column_list]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into 3 parts: train/validation/test with 60%/20%/20% distribution. Use `train_test_split` function for that with `random_state=1`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val = df.copy()\n",
    "y_train_val = X_train_val.pop('y')\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=1)\n",
    "\n",
    "# Further split the train+validation set into train and validation sets (75% / 25%, which is 60% / 20% of the original data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=1)\n",
    "\n",
    "print(f\"Train set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: ROC AUC feature importance\n",
    "\n",
    "ROC AUC could also be used to evaluate feature importance of numerical variables. \n",
    "\n",
    "Let's do that\n",
    "\n",
    "* For each numerical variable, use it as score and compute AUC with the `y` variable\n",
    "* Use the training dataset for that\n",
    "\n",
    "\n",
    "If your AUC is < 0.5, invert this variable by putting \"-\" in front\n",
    "\n",
    "(e.g. `-df_train['engine_hp']`)\n",
    "\n",
    "AUC can go below 0.5 if the variable is negatively correlated with the target variable. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive.\n",
    "\n",
    "Which numerical variable (among the following 4) has the highest AUC?\n",
    "\n",
    "- `balance`\n",
    "- `day`\n",
    "- `duration`\n",
    "- `previous`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "numerical_columns = [\"balance\", \"day\", \"duration\", \"previous\"]\n",
    "auc_scores = []\n",
    "\n",
    "for col in numerical_columns:\n",
    "    score = roc_auc_score(y_train, X_train[col])\n",
    "    if score < 0.5:\n",
    "        score = roc_auc_score(y_train, -X_train[col])\n",
    "    auc_scores.append((col, score))\n",
    "\n",
    "for col, score in auc_scores:\n",
    "    print(\"AUC score\", f\"{col}: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Training the model\n",
    "\n",
    "Apply one-hot-encoding using `DictVectorizer` and train the logistic regression with these parameters:\n",
    "\n",
    "```python\n",
    "LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "```\n",
    "\n",
    "What's the AUC of this model on the validation dataset? (round to 3 digits)\n",
    "\n",
    "- 0.69\n",
    "- 0.79\n",
    "- 0.89\n",
    "- 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dv = DictVectorizer()\n",
    "\n",
    "X_train_encoded = dv.fit_transform(X_train.to_dict(orient=\"records\"))\n",
    "X_val_encoded = dv.transform(X_val.to_dict(orient=\"records\"))\n",
    "\n",
    "model = LogisticRegression(solver=\"liblinear\", C=1.0, max_iter=1000)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "y_val_pred = model.predict(X_val_encoded)\n",
    "y_val_pred_proba = model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_val, y_val_pred_proba)\n",
    "\n",
    "print(f\"AUC score: {auc_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Precision and Recall\n",
    "\n",
    "Now let's compute precision and recall for our model.\n",
    "\n",
    "* Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "* For each threshold, compute precision and recall\n",
    "* Plot them\n",
    "\n",
    "At which threshold precision and recall curves intersect?\n",
    "\n",
    "* 0.265\n",
    "* 0.465\n",
    "* 0.665\n",
    "* 0.865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "thresholds = np.arange(0.0, 1, 0.01)\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_val_pred_proba >= threshold).astype(int)\n",
    "    y_pred = np.where(y_pred == 0, 'no', 'yes')\n",
    "\n",
    "    precision = precision_score(y_val, y_pred, pos_label='yes')\n",
    "    recall = recall_score(y_val, y_pred, pos_label='yes')\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thresholds, precisions, label='Precision')\n",
    "plt.plot(thresholds, recalls, label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: F1 score\n",
    "\n",
    "Precision and recall are conflicting - when one grows, the other goes down. That's why they are often combined into the F1 score - a metrics that takes into account both\n",
    "\n",
    "This is the formula for computing F1:\n",
    "\n",
    "$$F_1 = 2 \\cdot \\cfrac{P \\cdot R}{P + R}$$\n",
    "\n",
    "Where $P$ is precision and $R$ is recall.\n",
    "\n",
    "Let's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01\n",
    "\n",
    "At which threshold F1 is maximal?\n",
    "\n",
    "- 0.02\n",
    "- 0.22\n",
    "- 0.42\n",
    "- 0.62\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "thresholds = np.arange(0.0, 1, 0.01)\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_val_pred_proba >= threshold).astype(int)\n",
    "    y_pred = np.where(y_pred == 0, 'no', 'yes')\n",
    "\n",
    "    f1_score_val = f1_score(y_val, y_pred, pos_label='yes')\n",
    "    f1_scores.append(f1_score_val)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thresholds, f1_scores, label='f1_score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('F1 Score vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: 5-Fold CV\n",
    "\n",
    "\n",
    "Use the `KFold` class from Scikit-Learn to evaluate our model on 5 different folds:\n",
    "\n",
    "```\n",
    "KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "```\n",
    "\n",
    "* Iterate over different folds of `df_full_train`\n",
    "* Split the data into train and validation\n",
    "* Train the model on train with these parameters: `LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)`\n",
    "* Use AUC to evaluate the model on validation\n",
    "\n",
    "How large is standard deviation of the scores across different folds?\n",
    "\n",
    "- 0.0001\n",
    "- 0.006\n",
    "- 0.06\n",
    "- 0.26\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "auc_scores = []\n",
    "dv = DictVectorizer()\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_val):\n",
    "\n",
    "    X_train, X_val = X_train_val.iloc[train_index], X_train_val.iloc[val_index]\n",
    "    y_train, y_val = y_train_val.iloc[train_index], y_train_val.iloc[val_index]\n",
    "\n",
    "    X_train_encoded = dv.fit_transform(X_train.to_dict(orient=\"records\"))\n",
    "    X_val_encoded = dv.transform(X_val.to_dict(orient=\"records\"))\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "\n",
    "    y_val_pred = model.predict_proba(X_val_encoded)[:, 1]\n",
    "    auc = roc_auc_score(y_val, y_val_pred)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "std_dev_auc = np.std(auc_scores)\n",
    "print(f'Standard Deviation of AUC: {std_dev_auc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Hyperparameter Tuning\n",
    "\n",
    "Now let's use 5-Fold cross-validation to find the best parameter `C`\n",
    "\n",
    "* Iterate over the following `C` values: `[0.000001, 0.001, 1]`\n",
    "* Initialize `KFold` with the same parameters as previously\n",
    "* Use these parameters for the model: `LogisticRegression(solver='liblinear', C=C, max_iter=1000)`\n",
    "* Compute the mean score as well as the std (round the mean and std to 3 decimal digits)\n",
    "\n",
    "Which `C` leads to the best mean score?\n",
    "\n",
    "- 0.000001\n",
    "- 0.001\n",
    "- 1\n",
    "\n",
    "If you have ties, select the score with the lowest std. If you still have ties, select the smallest `C`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [0.000001, 0.001, 1]\n",
    "results = {}\n",
    "\n",
    "for C in C_values:\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    auc_scores = []\n",
    "    dv = DictVectorizer()\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train_val):\n",
    "        X_train, X_val = X_train_val.iloc[train_index], X_train_val.iloc[val_index]\n",
    "        y_train, y_val = y_train_val.iloc[train_index], y_train_val.iloc[val_index]\n",
    "\n",
    "        X_train_encoded = dv.fit_transform(X_train.to_dict(orient=\"records\"))\n",
    "        X_val_encoded = dv.transform(X_val.to_dict(orient=\"records\"))\n",
    "\n",
    "        model = LogisticRegression(solver='liblinear', C=C, max_iter=1000)\n",
    "        model.fit(X_train_encoded, y_train)\n",
    "\n",
    "        y_val_pred = model.predict_proba(X_val_encoded)[:, 1]\n",
    "        auc = roc_auc_score(y_val, y_val_pred)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "    results[C] = (mean_auc, std_auc)\n",
    "\n",
    "\n",
    "for C, (mean_auc, std_auc) in results.items():\n",
    "    print(f\"C: {C}, Mean AUC: {mean_auc:.3f}, Std: {std_auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://courses.datatalks.club/ml-zoomcamp-2024/homework/hw04\n",
    "* If your answer doesn't match options exactly, select the closest one\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmzoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
